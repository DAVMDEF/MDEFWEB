<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="generator" content="Github Pages">
    <meta name="description" content="MDEF documentation site for Your Name">
    <title>Your name - MDEF</title>
    <link rel="stylesheet" href="../style.css">
    <link rel="icon" href="../images/favicon.svg">
  </head>

  <!DOCTYPE html>
  <html lang="en">
  <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <title>Sliding Hamburger Menu</title>
      <link rel="stylesheet" href="style.css">
  </head>

  <head>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
</head>


  <body>
  
      <!-- Menu Container for both icon and menu -->
      <div class="menu-container">
          <!-- Hamburger Icon -->
          <div class="menu-icon" id="menuIcon">
              &#9776; <!-- Hamburger icon -->
          </div>
  
          <!-- Sliding Navbar -->
          <div class="navbar" id="navMenu">
              <div class="navbar-inner">
                  <a href="../index.html">home</a>
                  <a href="../about.html">about / work</a>
                  <a href="../term1/index.html">term 1</a>
                  <a href="../term2/index.html">term 2</a>
                  <a href="../term3/index.html">term 3</a>
                  <a href="../master-thesis.html">master thesis</a>                  
              </div>
          </div>
      </div>
  
  </body>
  </html>
  

  <body>
    
    <div class="content">

      <h1 style="margin-bottom: 30px;">04. extended intelligences</h1>

      <h2 style="margin-bottom: 21px;">reflection</h2>

      <p style="margin-bottom: 21px;">The seminar was an interesting continuation of the previous module where we understood the behind the scenes implications of ai use, ending with a brief introduction to the different tools and datasets available. This time extending the theory into a more tangible application of the technology exploring how it can be implemented into objects. The combination of AI engines and barduino programming gave an insight into the possibility of extending the way we use artifacts to collect and interpret data. </p>

      <img style="margin-bottom: 21px; width: 12%;" src="../images/term 2/ext int ll/s1.jpeg">
      <img style="margin-bottom: 21px; width: 12%;" src="../images/term 2/ext int ll/s2.jpeg">
      <img style="margin-bottom: 21px; width: 12%;" src="../images/term 2/ext int ll/s3.jpeg">
      <img style="margin-bottom: 21px; width: 12%;" src="../images/term 2/ext int ll/s4.jpeg">
      <img style="margin-bottom: 21px; width: 12%;" src="../images/term 2/ext int ll/s5.jpeg">
      <img style="margin-bottom: 21px; width: 12%;" src="../images/term 2/ext int ll/s6.jpeg">
      <img style="margin-bottom: 21px; width: 12%;" src="../images/term 2/ext int ll/s7.jpeg">
      <img style="margin-bottom: 21px; width: 12%;" src="../images/term 2/ext int ll/s8.jpeg">

      <p style="margin-bottom: 21px;">The first exercise using DOTTOD camera application provided a quick and easy understanding of the steps required for the machine to receive input, analice it and through prompt crafting from the user, then generate an interpretation of the original information. From a personal point of view it was important to understand the process and restrictions it involves, like planning ahead the input that is going to be fed to the tool in order to allow the best possible analisis. It became clear that in order to make the best of this type of tool the input given to it must be intentionally produced in all its elements knowing what the machine is able to take from it. If in the alternative scenario where the input cannot be intentional, for example a previously taken photograph, then the other relevant learning from with this exercise comes into play. This second aspect is also from a personal opinion the most relevant aspect of the exercise, prompt engineering. 
      </p>

      <p style="margin-bottom: 21px;">The second exercise proved to be confirmation of this previous conclusion. Prompt engineering is essential for an optimal use of these tools. From the little experience and understanding I personally have in the subject I would go as far as suggesting that half of the job of generative ai tools is done in the prompt definition. The experience using the Modmatrix interface added a few steps to the previous exercise and allowed further exploration on how to work collaboratively with the machine to produce an outcome. It was interesting to explore how to collaborate with the tool in making it understand the desired analysis of the given input and then crafting an output. </p>
      
      <style>
        .centered-image {
            display: block;
            margin: 0 auto;
        }
    </style>
    
    <img style="margin-bottom: 21px; width: 60%;" src="../images/term 2/ext int ll/screen 1.png" alt="Centered Image" class="centered-image">
    
      
      <p style="margin-bottom: 21px;">The seminar concluded with a final project proposal to apply a combination of arduino programing and generative ai to an artifact. The concept developed by the group for this exercise was an exploration of how we can communicate with a given space through light. The idea being to experiment with how the machine can perceive the mood in the room and then communicate this perception through an emotion in the shape of a melody. </p>

      <p style="margin-bottom: 21px;">For this experiment taking into consideration time limitations, it was decided that the way the machine would perceive the space would be through light and temperature, two sensors already available in the barduino board. Throughout the process the complexity of the “simple task” we chose became evident. Resolution not only in the reduction of the melody outcome to a simple series of bips, but also in having to reduce the amount of input feeded to the machine to just light sensing. </p>

      <p style="margin-bottom: 21px;">The result was not the expected. In the end the artifact wasn't able to translate the characteristics of the light parameters into any sort of coherent response. The groups conclusion was that, since the barduino programing and circuits seem to be correctly made, the problem lied on the prompt given to the ai tool to produce an output. </p>

      <body>

        <!-- Embedded YouTube video -->
        <iframe style="margin: auto" width="560" height="315" src="https://www.youtube.com/embed/dSMhfmTq7gI" 
                title="YouTube video player" 
                frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                allowfullscreen></iframe>
    
    </body>

      <p style="margin-bottom: 21px;">To highlight my personal contribution to the project I must clarify that my nonexistent arduino skills were shadowed by my teammates’, and so I can say that the most of it came in the concept ideation part and collectively tempering with the prompt given to the machine. Which failed.</p>

      <p style="margin-bottom: 21px;">Nevertheless, this project gave a very interesting insight of the capabilities and applications of mixed uses of these technologies. The artifact developed is now under consideration to be adapted to the main project of the master in order to become an alternative to sense a specific space and define its characteristics. As far as the relevance this experience brought to the masters project, one aspect is the possibility to use these tools to re-interpret data and project it in a sort of “impartial” way by “collaborating” with them.</p>

